{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 论文解读\n",
    "\n",
    "&emsp;&emsp;之前学习Transformer的时候，主要注意的是attention的机制和论文的模型结构，并且对tensorflow的代码大致了解过，有些细节没有详细研究过，论文有些trick也没有注意到，现在重新整理一下思路，并结合代码进行解读。\n",
    "\n",
    "### 1. 本次整理发现的细节\n",
    "\n",
    "1. residual dropout:apply dropout to the output of each sub-layer,before it is added to the sub-layer input and normalized, In addition,we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
    "2. label smoothing: \n",
    "3. embedding:论文中说we share tghe weight matrix between the two embedding layers,那是因为它把encoder的预料和decoder的预料合并在一起了，我们实现的时候可以跳过这个trick。\n",
    "\n",
    "### 2. Transformer 架构\n",
    "\n",
    "&emsp;&emsp;Transformer架构也使用的是encoder-decoder结构，如下图所示：\n",
    "\n",
    "<img src=\"./pic/transformer.png\" width = 30% height = 30% div align=center />\n",
    "\n",
    "- 输入序列经过word embedding 和 positional embedding相加后，输入到encoder\n",
    "- 输出序列经过word embedding 和 positional embedding相加后，输入到decoder\n",
    "- decoder输出的结果经过 linear ，softmax后，计算下一个预测的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Encoder\n",
    "\n",
    "&emsp;&emsp;encoder由6层相同的层组成，每层分别由两部分组成：\n",
    "\n",
    "- 第一部分是multi-head attention\n",
    "- 第二部分是position-wise feed-forward network\n",
    "\n",
    "&emsp;&emsp;这两个部分，都有一个residual connection，然后连接一个 layer normalization\n",
    "\n",
    "#### 2.2 Decoder\n",
    "\n",
    "&emsp;&emsp;decoder和encoder类似，每一层包含以下三个部分:\n",
    "\n",
    "- 1.multi-head self-attention \n",
    "- 2.multi-head context-attention\n",
    "- 3.position-wise feed-forward network \n",
    "\n",
    "&emsp;&emsp;这三个部分，都有一个residual connection，然后连接一个 layer normalization\n",
    "\n",
    "#### 2.3 Attention\n",
    "\n",
    "&emsp;&emsp;Attention机制就是对于某个时刻的输出y,它在输入x上各个部分的注意力，就是权重。\n",
    "\n",
    "<img src=\"./pic/attention.jpg\" width=30% height=30% div align=center/>\n",
    "\n",
    "&emsp;&emsp;对于输入序列的hidden state $h_i$和输出序列的hidden state $s_t$，计算attention_score方法有三种，在transformer中采用的是乘性注意力机制，即使两个hidden state进行点积运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 self-Attention\n",
    "\n",
    "&emsp;&emsp;self-attention就是输出序列就是输入序列，就是在word embedding的时候，不光考虑自己这个单词，也需要考虑这句话中其他的单词，这里采用乘性注意力机制的一个好处就是，一个向量点积自己一定是最大的，因此self-attention机制计算的得分总是关注自己的得分最大，也符合我们单词编码的自觉，一个token的编码肯定是和自己最相关的。\n",
    "\n",
    "##### 2.3.2 context-Attention\n",
    "\n",
    "&emsp;&emsp;context-attention 就是在decode的t时刻，获取该时刻在encoder的context 向量，此时的attention是decoder和encoder各个时刻的token的attention得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3 Scaled dot-product Attention\n",
    "\n",
    "&emsp;&emsp;论文中的计算公式如下：\n",
    "\n",
    "<img src=\"./pic/scaled_attention.jpg\" width=40% height=40% div align=center />\n",
    "\n",
    "&emsp;&emsp;结合我的理解对query,key,value进行简单的讲解，目前存在一个字典，dict = {key:value},key和value都是一个向量，目前来一个查询query,我们想知道query对应的value是多少，但是在dict中没有query,因此最简单的做法就是将字典中的value进行average作为query对应的value,但是这样对任意一个query,得到的value都是一样的，因此这种处理方式不好，我们希望可以通过计算query和key之间的sim得分，来得到一个dict中所有value的加权权重，这样给query的value具有一定的信服力。\n",
    "\n",
    "&emsp;&emsp;那么结合transformer，具体说明以下query,key,value是什么？\n",
    "\n",
    "- 1.在encoder的self-attention中，query,key,value都来自上一层encoder的输出，对于第一层encoder，它们就是word embedding + positional embedding\n",
    "- 2.在decoder的self-attention中，query,key,value都来自上一层decoder的输出，对于第一层decoder，它们就是word embedding + positional embedding，但是对于decoder,不希望它能获得下一个时刻的信息，因此需要进行sequence masking\n",
    "- 3.在encoder-decoder attetnion中，query来自decoder的上一层的输出，key,value来自encoder的输出。\n",
    "\n",
    "##### 2.3.4 Multi-head Scaled dot-product Attention\n",
    "\n",
    "&emsp;&emsp;论文提到，将query,key,value 通过一个线性映射之后，分成h份，然后对每一份进行scaled dot-product attention，然后把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。\n",
    "\n",
    "&emsp;&emsp;论文中的说明如下：\n",
    "\n",
    "<img src=\"./pic/multi-head.jpg\" width=80% height=80% div align=center />\n",
    "\n",
    "&emsp;&emsp;scaled dot-product 和 multi-head scaled dot-product的图解\n",
    "\n",
    "<img src=\"./pic/attention_crontast.jpg\" width=80% height=80% div align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Residual connection\n",
    "\n",
    "&emsp;&emsp;就是残差连接，假设网络中某个层对输入x作用后的输出是F(x),那么增加residual connection之后就是 x + F(x)。\n",
    "\n",
    "&emsp;&emsp;残差连接好处就是在反向传播的过程中，因为增加了常数项1，所以不会造成梯度消失。\n",
    "\n",
    "#### 2.5 layer normalization\n",
    "\n",
    "&emsp;&emsp;layer normalization是在每一个样本上计算均值和方差，公式如下：\n",
    "\n",
    "$$LN(x_i) = \\alpha * \\frac{x_i - u_L}{\\sqrt{\\delta_l^2 + e}} + \\beta$$\n",
    "\n",
    "&emsp;&emsp;其中 $\\alpha$ 和 $\\beta$ 是可学习的参数。\n",
    "\n",
    "#### 2.6 mask\n",
    "\n",
    "&emsp;&emsp;mask就是掩码的意思，就是对某些值进行掩盖，使其不产生效果，在transformer中涉及到两种mask，padding mask 和 sequence mask。\n",
    "\n",
    "##### 2.6.1 padding mask\n",
    "\n",
    "&emsp;&emsp;每个批次的输入序列长度不一样，但是为了保证输入长度一样，对长度不够的句子用padding token进行填补，因为这些填补的位置，其实是没有什么意义的，所以attention机制不应该把注意力放在这些位置上，因此需要进行处理。\n",
    "\n",
    "&emsp;&emsp;具体做法就是在这些padding位置上加一个非常大的负数，经过softmax后，这些位置的概率就会接近0。\n",
    "\n",
    "##### 2.6.2 sequence mask\n",
    "\n",
    "&emsp;&emsp;sequence mask 是为了使得decoder不能看见未来的信息，即对于一个序列，在t时刻的时候，解码的输出只能依赖于t时刻之前的输出，因此需要将t时刻之后的信息隐藏起来。也就是在t时刻，只能看到1 ... t-1时刻的信息。\n",
    "\n",
    "#### 2.7 positional encoding\n",
    "\n",
    "&emsp;&emsp;目前的transformer架构缺点东西，它对序列的顺序没有约束，没有positional encoding，它就是词袋模型，可能一句话里面的token 都预测正确，但是顺序预测不正确，论文中提出了positional encoding。\n",
    "\n",
    "&emsp;&emsp;论文采用正余弦函数进行编码，公式如下\n",
    "\n",
    "$$PE(pos,2i) = sin(pos/10000^{2i/d_model})$$\n",
    "\n",
    "$$PE(pos,2i+1) = cos(pos/10000^{2i/d_model})$$\n",
    "\n",
    "&emsp;&emsp;其中pos是指词语在序列中的位置，i是维度，上边这个公式不是很直观，其实就是将维度i分为奇数和偶数。假设$d_model$=4，那么可以分成两个pair:(0,1),(2,3),其中一个pair中的周期是相同的。\n",
    "\n",
    "&emsp;&emsp;上面的位置对应绝对位置编码，但是词语的相对位置也非常重要\n",
    "\n",
    "$$sin(a + b) = sin(a)cos(b) + cos(a)sin(b)$$\n",
    "\n",
    "$$cos(a + b) = cos(a)cos(b) - sin(a)sin(b)$$\n",
    "\n",
    "&emsp;&emsp;上面的公式说明，对于词汇之间的位置偏移k，PE(pos+k)可以表示为PE(pos)和PE(k)的组合形式。\n",
    "\n",
    "#### 2.8 word embedding\n",
    "\n",
    "&emsp;&emsp;in the embedding layers,multiply those weights by $\\sqrt{d_model}$\n",
    "\n",
    "#### 2.9 position-wise feed-forward network\n",
    "\n",
    "&emsp;&emsp;就是一个全连接层，包含两个线性变换和一个非线性函数(relu),公式如下：\n",
    "\n",
    "$$FFN(x) = max(0,xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pytorch 实现\n",
    "\n",
    "#### 3.1 attention实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    '''\n",
    "    query,key,value size: [batch,seq_len,d_k]\n",
    "    mask: [batch_size,seq_len,seq_len]\n",
    "    return:\n",
    "        context: [batch,seq_len,d_k]\n",
    "        attention_weight : [batch,seq_len,seq_len]\n",
    "    '''\n",
    "    d_k = query.size(-1)\n",
    "    # [batch,seq_len,seq_len]\n",
    "    scores = torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores.masked_fill(mask==0,-1e9)\n",
    "    p_attn = F.softmax(scores,dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # [batch,seq_len,d_k]\n",
    "    context_vec = torch.matmul(p_attn,value)\n",
    "    return context_vec,p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 MultiHead attention实现\n",
    "\n",
    "$$MultiHead(Q,K,V) = Concat(head_1,head_2,...,head_h)W^O$$\n",
    "\n",
    "$$head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi head attention\n",
    "'''\n",
    "MultiHead(Q,K,V) = Concat(head1,...headh)W^O\n",
    "head_i\n",
    "\n",
    "'''\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,h,d_model,dropout_rate = 0.1):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model / h\n",
    "        self.h = h\n",
    "        self.query_linear = nn.Linear(d_model,d_model)\n",
    "        self.key_linear = nn.Linear(d_model,d_model)\n",
    "        self.value_linear = nn.Linear(d_model,d_model)\n",
    "        self.proj_linear = nn.Linear(d_model,d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        '''\n",
    "        mask : [batch_size,seq_len,seq_len]\n",
    "        '''\n",
    "        batch_size = query.size(0)\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "        \n",
    "        # split by heads\n",
    "        # [batch_sizes * heads,seq_len,d_k]\n",
    "        query = query.view(batch_size * self.h,-1,self.d_k)\n",
    "        key = key.view(batch_size * self.h,-1,self.d_k)\n",
    "        value = value.view(batch_size * self.h,-1,self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # [batch_size * h,seq_len,seq_len]\n",
    "            mask = mask.repeat(self.h,1,1)\n",
    "        \n",
    "        # context [batch_size * h,seq_len,d_k]\n",
    "        context,self.attn = attention(query,key,value,mask,self.dropout)\n",
    "        context = context.contiguous().view(batch_size,-1,self.d_k * self.h)\n",
    "        return self.porj_linear(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Layer normalization的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.alpha * (x-mean)/(std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Position-wise Feed-Forward Networks的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout_rate = 0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model,d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff,d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    def forward(self,x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Embedding and softmax的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super(Embeddings,self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.word_embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Positional Encoding的实现\n",
    "\n",
    "- 偶数列 dim i:\n",
    "$$PE(pos,i) = sin(pos/10000^{i/d_{model}})$$\n",
    "\n",
    "- 奇数列 dim i:\n",
    "$$PE(pos,i) = cos(pos/10000^{(i-1)/d_{model}})$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,dropout_rate=0.1,max_len = 500):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # [max_len,d_model]\n",
    "        positional_encoding = np.array([\n",
    "            [pos/np.pow(10000,2.0 * (j//2)/d_model) for j in range(d_model)]\n",
    "        ] for pos in range(max_len))\n",
    "        # 偶数列使用sin,奇数列使用cos\n",
    "        positional_encoding[:,0::2] = np.sin(positional_encoding[:,0::2])\n",
    "        positional_encoding[:,1::2] = np.cos(positional_encoding[:,1::2])\n",
    "        \n",
    "        # [1,max_len,seq_len]\n",
    "        pe = torch.from_numpy(positional_encoding).unsqueeze(0)\n",
    "        # 在内存中定义一个常量，同时，模型保存和加载的时候可以写入和读出。\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self,x):\n",
    "        x = x + Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking shi\n",
    "\n",
    "# padding mask \n",
    "def padding_mask(seq_query,seq_key):\n",
    "    '''\n",
    "    function: padding用到三个地方，encoder,decoder,encoder-decoder\n",
    "    input:\n",
    "        seq_query: [batch_size,query_len]\n",
    "        seq_key: [batch_size,key_len]\n",
    "    return : [batch_size,query_len,key_len]\n",
    "    '''\n",
    "    query_len = seq_query.size(1)\n",
    "    # pad is 0\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    # [batch,seq_len,key_len]\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)\n",
    "    return pad_mask\n",
    "\n",
    "# sequence mask\n",
    "# 产生一个上三角矩阵，上三角的值全为1，下三角的值全为0，对角线也是0\n",
    "def sequence_mask(seq):\n",
    "    '''\n",
    "    mask future info\n",
    "    input: seq: [batch,seq_len]\n",
    "    return mask : [batch_size,seq_len,seq_len]\n",
    "    '''\n",
    "    batch_size,seq_len = seq.size()\n",
    "    mask = torch.triu(torch.ones((seq_len,seq_len),dtype=torch.unit8),diagonal=1)\n",
    "    mask = mask.unsqueeze(0).expand(batch_size,1,1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 SublayerConnection\n",
    "\n",
    "&emsp;&emsp;the output of each sub-layer is LayerNorm(x + sublayer(x)),where sublayer(x) is the function implemented by the sub-layer itself,we apply dropout to the output of each sub-layer,before it is added to the sub-layer inpuyt and normalized\n",
    "\n",
    "&emsp;&emsp;output = LayerNorm(x + dropout(subLayer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self,size,dropout_rate = 0.1):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 其他细节和参考文献\n",
    "\n",
    "&emsp;&emsp;目前Transformer 大致的细节都以代码的形式实现了，还有mask部分没有实现，接下来在代码详细写一下，然后跑一下程序。\n",
    "\n",
    "**参考文献**\n",
    "\n",
    "[Transformer模型笔记](https://zhuanlan.zhihu.com/p/39034683)\n",
    "\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "[Transformer的PyTorch实现](https://blog.csdn.net/stupid_3/article/details/83184691)\n",
    "\n",
    "[Transformer解读（论文 + PyTorch源码）](https://blog.csdn.net/Magical_Bubble/article/details/89083225#7_Tricks_72)\n",
    "\n",
    "[Transform详解(超详细) Attention is all you need论文](https://zhuanlan.zhihu.com/p/63191028)\n",
    "\n",
    "[碎碎念：Transformer的细枝末节](https://zhuanlan.zhihu.com/p/60821628)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
