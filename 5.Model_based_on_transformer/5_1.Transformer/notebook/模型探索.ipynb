{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch normalization\n",
    "&emsp;&emsp;batch normalization 对隐藏层的数据进行正态分布标准化，由于标准化后可能影响神经网络的表达能力，normalize后的数据再使用缩放系数$\\gamma$和平移系数$\\beta$进行缩放和平移。其中$\\gamma$和$\\beta$参数需要进行反向传播，使得处理后的数据达到最佳的使用效果。\n",
    "\n",
    "$$\\mu_{\\beta} = \\frac{1}{m}\\sum_{i=1}^{m}{x_{i}} \\tag{1-1}$$\n",
    "\n",
    "$$\\delta_{\\beta}^{2} = \\frac{1}{m}\\sum_{i=1}^{m}{(x_{i} - \\mu_{\\beta})^{2}} \\tag{1-2}$$\n",
    "\n",
    "$$x_{i}^{-} = \\frac{x_{i} - \\mu_{\\beta}}{\\sqrt{\\delta_{\\beta}^{2} + e}} \\tag{1-3}$$\n",
    "\n",
    "$$y_{i} = \\gamma x_{i}^{-} + \\beta \\tag{1-4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;batch normalization特点\n",
    "1. 依赖batch_size\n",
    "2. 对处理序列化数据的网络不太使用\n",
    "3. 只在训练的时候用，inference的时候用不到\n",
    "\n",
    "&emsp;&emsp;在训练最后一个epoch时，要对这一epoch所有的训练样本的均值和标准差进行统计，这样在测试数据过来的时候，使用训练样本的标准差的期望和均值的期望对测试数据进行归一化，注意标准差这里使用的期望是无偏估计。\n",
    "\n",
    "$$E[x] = E_{\\beta}[\\mu_{\\beta}]$$\n",
    "\n",
    "$$Var[x] = \\frac{m}{m-1}{E_{\\beta}[\\delta_{\\beta}^{2}]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer normalization\n",
    "\n",
    "&emsp;&emsp;layer normalization 比较适合用于RNN和单条样本的训练和预测。\n",
    "\n",
    "$$\\mu = \\frac{1}{H}\\sum_{i=1}^{H}{x_{i}} \\tag{2-1}$$\n",
    "\n",
    "$$\\delta = sqrt{\\frac{1}{H}\\sum{i=1}{H}{(x_{i} - \\mu)^{2}}} \\tag{2-2}$$\n",
    "\n",
    "$$y = g \\odot \\frac{x-\\mu}{\\sqrt{\\delta_{2} + e} + b} \\tag{2-3}$$\n",
    "\n",
    "&emsp;&emsp;其中g和b是可学习的参数,$\\odot$为element-wise乘法\n",
    "\n",
    "&emsp;&emsp;上面的公式中H 是一层神经元的个数，这里一层网络共享一个均值和方差，不同训练样本对应不同的均值和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of each sub-layer is LayerNorm(x + sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    '''\n",
    "    Produce N identical layers\n",
    "    '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Construct a layernorm module (see citation for details)\n",
    "    '''\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(features))\n",
    "        self.b = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.a * (x-mean)/(std + self.eps) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    core encoder is a stack of n layers\n",
    "    '''\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        '''\n",
    "        pass the input (and mask) through each layer in turn\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm\n",
    "    note for code simplicity the norm is first as oppsed to last\n",
    "    '''\n",
    "    def __init__(self,size,dropout):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        '''\n",
    "        apply residual connection to any sublayer with the same size\n",
    "        '''\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    encoder is made up of self-attn and feed forward\n",
    "    '''\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        x = self.sublayer[0](x,lambda x:self.self_attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    '''\n",
    "    mask out subsequent positions \n",
    "    target word(row) is allowed to look at (column)\n",
    "    '''\n",
    "    attn_shape = (1,size,size)\n",
    "    # 其中target word 只考虑target word 以及之前的word\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    generic N layer decoder with masking\n",
    "    '''\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,memory,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    decoder is made of self-attn,src-attn and feed forward\n",
    "    '''\n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.scr_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),3)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n",
    "        x = self.sublayer[1](x,lambda x:self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x,self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    'compute scaled dot product attention'\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query,key.transpose(-2,-1))\\math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ones = np.ones((1,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(test_ones,k=1).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
