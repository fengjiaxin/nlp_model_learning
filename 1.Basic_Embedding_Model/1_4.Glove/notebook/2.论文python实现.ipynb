{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据提供的预料库，构建此表\n",
    "def build_vocab(corpus):\n",
    "    '''\n",
    "    build a vocabulary with word frequencies for an entire corpus\n",
    "    return a dic 'w -> (i,f)'\n",
    "    '''\n",
    "    print('building vocab from corpus')\n",
    "\n",
    "    vocab = Counter()\n",
    "    for line in corpus:\n",
    "        tokens = line.split()\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    print('Done building vocab from corpus')\n",
    "\n",
    "    return {word:(i,freq) for i,(word,freq) in enumerate(vocab.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建共现矩阵\n",
    "def build_cooccur(vocab,corpus,window_size=5,min_count=None):\n",
    "    '''\n",
    "    buil a word co-occurrence list for the given corpus\n",
    "\n",
    "    this function is a tuple generator,where each element \\\n",
    "    (reapresenting a coocuurence parir) is of the form (i_main,i_context,cooccurrence)\n",
    "\n",
    "    :param vocab:\n",
    "    :param corpus:\n",
    "    :param window_size:\n",
    "    :param min_count:\n",
    "    :return:\n",
    "    '''\n",
    "    vocab_size = len(vocab)\n",
    "    id2word = dict((i,word) for word,(i,_) in vocab.items())\n",
    "\n",
    "    # collect cooccurrences internally as a sparse matrix for passable\n",
    "    # indexing speed;we will convert into a list later\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size,vocab_size),dtype=np.float64)\n",
    "\n",
    "    for i,line in enumerate(corpus):\n",
    "        if i % 1000 == 0:\n",
    "            print('building cooccurrence matrix: on line %i',i)\n",
    "\n",
    "        tokens = line.strip().split()\n",
    "        token_ids = [vocab[word][0] for word in tokens]\n",
    "\n",
    "        for center_i,center_id in enumerate(token_ids):\n",
    "            # collect all word ids in left window of center word\n",
    "            context_ids = token_ids[max(0,center_i - window_size):center_i]\n",
    "            contexts_len = len(context_ids)\n",
    "\n",
    "            for left_i,left_id in enumerate(context_ids):\n",
    "                # distance from center word\n",
    "                distance = contexts_len - left_i\n",
    "\n",
    "                # Weight by inverse of distance between words\n",
    "                increment = 1.0/float(distance)\n",
    "\n",
    "                # build co-occurrence matrix symmetrically\n",
    "                cooccurrences[center_id,left_id] += increment\n",
    "                cooccurrences[left_id,center_id] += increment\n",
    "\n",
    "    # now yield our tuple sequence\n",
    "    for i,(row,data) in enumerate(zip(cooccurrences.rows,cooccurrences.data)):\n",
    "        if min_count is not None and vocab[id2word[i]][1] < min_count:\n",
    "            continue\n",
    "\n",
    "        for data_idx,j in enumerate(row):\n",
    "            if min_count is not None and vocab[id2word[j]][1] < min_count:\n",
    "                continue\n",
    "            yield i,j,data[data_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 重要公式\n",
    "\n",
    "$$w_i^{T} w_j + b_i + b_j = log(X_{i,j}) \\tag{1}$$\n",
    "\n",
    "$$J = \\sum_{i=1}^{V} \\sum_{j=1}^{V}f(X_{i,j})(w_i^T w_j + b_i + b_j - log(X_{i,j}))^2 \\tag{2}$$\n",
    "\n",
    "$$ f(x)=\\begin{cases}\n",
    "(x/x_{max})^{\\alpha} & if x < x_{max} \\\\\n",
    "1 & otherwise\n",
    "\\end{cases} \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = \\sum_{i=1}^{V} \\sum_{j=1}^{V}f(X_{i,j})(w_i^T w_j + b_i + b_j - log(X_{i,j}))^2 \\tag{2}$$\n",
    "\n",
    "##### 求导公式\n",
    "\n",
    "$$\\nabla{w_i}J = \\sum_{j=1}^{V}f(X_{i,j})(w_i^T w_j + b_i + b_j - log(X_{i,j})) * w_j \\tag{4}$$ \n",
    "\n",
    "$$\\frac{\\alpha J}{\\alpha b_i} = \\sum_{j=1}^{V}f(X_{i,j}((w_i^T w_j + b_i + b_j - log(X_{i,j})) \\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_glove(vocab,cooccurrences,vector_size=50,iterations=5,x_max=10,alpha=0.75,learning_rate=0.01,**kwargs):\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # 建立词向量 上半vocab_size存储i_main词向量，下半vocab_size存储i_context的词向量\n",
    "    W = ((np.random.rand(vocab_size * 2,vector_size) - 0.5)/float(vector_size + 1))\n",
    "\n",
    "    biases = ((np.random.rand(vocab_size * 2) - 0.5)/float(vector_size + 1))\n",
    "\n",
    "    # training is done via adaptive gradient descent(AdaGrad),to make this word we need to store\n",
    "    # the sum of squares of all previous gradients.\n",
    "\n",
    "    # initialize all squared gradient sums to 1 so that our inital adaptive learning rate is\n",
    "    # simply the global learning rate\n",
    "\n",
    "    gradient_squared = np.ones((vocab_size * 2, vector_size),dtype=np.float64)\n",
    "    gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n",
    "\n",
    "    data = []\n",
    "    for i_main, i_context, cur_freq in cooccurrences:\n",
    "        # i_main 代表 center_id\n",
    "        # i_context 代表 context_id\n",
    "        # cur_freq 代表 pair 出现的次数\n",
    "\n",
    "        tup = (\n",
    "            W[i_main],\n",
    "            W[i_context + vocab_size],\n",
    "            biases[i_main:i_main + 1],\n",
    "            biases[i_context + vocab_size:i_context + vocab_size + 1],\n",
    "            gradient_squared[i_main],\n",
    "            gradient_squared[i_context + vocab_size],\n",
    "            gradient_squared_biases[i_main:i_main + 1],\n",
    "            gradient_squared_biases[i_context + vocab_size:i_context + vocab_size + 1],\n",
    "            cur_freq\n",
    "        )\n",
    "        data.append(tup)\n",
    "\n",
    "    global_cost_list = []\n",
    "    # begin training by iteraltively calling the run_iter function\n",
    "    for i in range(iterations):\n",
    "        random.shuffle(data)\n",
    "        # begin train once iter\n",
    "        global_cost = 0\n",
    "        for (v_main,v_context,b_main,b_context,gradsq_W_main,gradsq_W_context,gradsq_b_main,gradsq_b_context,freq) in data:\n",
    "\n",
    "            # calculate weight function f(X_ij)\n",
    "            weight = ((cur_freq)/x_max) ** alpha if cur_freq < x_max else 1\n",
    "\n",
    "            # calculate cost_inner = w_i^Tw_j + b_i + b_j - log(X_{ij})\n",
    "            cost_inner = (v_main.dot(v_context) + b_main[0] + b_context[0] - np.log(cur_freq))\n",
    "\n",
    "            # compute cost\n",
    "            cost = weight * cost_inner\n",
    "            global_cost += cost\n",
    "\n",
    "            # with the cost calculated,we now need to compute gradients\n",
    "            # compute gradients for word vector terms\n",
    "            grad_main = weight * cost_inner * v_context\n",
    "            grad_context = weight * cost_inner * v_main\n",
    "\n",
    "            # compute gradients for bias terms\n",
    "            grad_bias_main = weight * cost_inner\n",
    "            grad_bias_context = weight * cost_inner\n",
    "\n",
    "            # now peerform adaptive updates\n",
    "            v_main -= (learning_rate * grad_main/np.sqrt(gradsq_W_main))\n",
    "            v_context -= (learning_rate * grad_context/np.sqrt(gradsq_W_context))\n",
    "\n",
    "            b_main -= (learning_rate * grad_bias_main/np.sqrt(gradsq_b_main))\n",
    "            b_context -= (learning_rate * grad_bias_context/np.sqrt(gradsq_b_context))\n",
    "\n",
    "            # update squared gradient sums\n",
    "            gradsq_W_main += np.square(grad_main)\n",
    "            gradsq_W_context += np.square(grad_context)\n",
    "            gradsq_b_main += grad_bias_main ** 2\n",
    "            gradsq_b_context += grad_bias_context ** 2\n",
    "        global_cost_list.append(global_cost)\n",
    "    return global_cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab from corpus\n",
      "Done building vocab from corpus\n",
      "building cooccurrence matrix: on line %i 0\n",
      "[14426.672101332308, 12272.679497731242, 10983.048528785379, 10060.370133115413, 9354.029158648644, 8789.492619283832, 8323.168814883915, 7927.9186288831925, 7585.301774498009, 7285.102503946788]\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/mini_content_process.txt'\n",
    "\n",
    "corpus = []\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip())\n",
    "\n",
    "vocab = build_vocab(corpus)\n",
    "cooccurrences = build_cooccur(vocab,corpus)\n",
    "global_cost_list = train_glove(vocab,cooccurrences,iterations=10)\n",
    "print(global_cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与word2vec的区别和联系\n",
    "\n",
    "1. skip_gram方法中最后一个的softmax后，希望其周边的词的概率越大越好，但是并没有考虑word_pairs之间的距离因素\n",
    "2. word2vec,glove都是考虑了共现矩阵的基础上建立模型，只是word2vec是一种预测模型，而glove是一种基于计数的模型\n",
    "\n",
    "#### 参考文献\n",
    "1. [论文分享-->GloVe: Global Vectors for Word Representation](https://blog.csdn.net/mr_tyting/article/details/80180780)\n",
    "2. [A GloVe implementation in Python](http://www.foldl.me/2014/glove-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
